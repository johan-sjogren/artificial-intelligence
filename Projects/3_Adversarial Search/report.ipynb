{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc90eee7-703d-45f7-9a20-b05b3a1205ee",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59cb04c-84bc-4d7b-a40a-0baad4c3eaed",
   "metadata": {},
   "source": [
    "## Advanced Search Techniques\n",
    "I choose to implement Monte Carlo Tree Search(MCTS) and compare it vs. an Iterative deepening with $\\alpha\\beta$ (ITERAB) pruning algorithm.\n",
    "The MCTS is the basic implementation with light rollout(i.e. pick a random action) and UCB(Upper Confidence Bound) for node/action selection. The one addition over the basic is that it reuses the tree in consecutive rounds.\n",
    "\n",
    "I updated the run_match.py so that one can feed both of the custom algorithms like so:  \n",
    "`python run_match.py -c SELF -o ITERAB -f -r 10 -p 2`  \n",
    "`python run_match.py -c ITERAB -o MINIMAX -f -r 10 -p 2`\n",
    "\n",
    "## Regarding \"Fair\"\n",
    "I have used the fair flag for the tests, but I am not sure that it is correct to call it fair. Why? The agents already take turns starting, so unfairness due to first mover advantage is already handled. What the fair flag does result in is really a check on how well agents perform in the initial rounds(and how well it can recover/build from that start). So it simply provides an input regarding whether or not the agent struggles with the first moves or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f1e414-cf53-433d-acc5-f64a8059d76f",
   "metadata": {},
   "source": [
    "### Result\n",
    "The evaluations where run for 25 rounds and with the -f (fair) setting (i.e. 50 + 50 games). The value in parentheses is from the fair rounds. Random and Greedy (marked with a *) where run with -f -r 10 setting.\n",
    "\n",
    "| Opponent | MCTS      | ITERAB|  \n",
    "|----------|----------:|---------:|  \n",
    "| Random*  | 90% (100%)| 80% (90%)|  \n",
    "| Greedy*  | 70% (70%) | 70% (90%)|  \n",
    "| MINIMAX  | 26% (40%) | 78% (84%)|  \n",
    "| MCTS     | 54% (58%) | 82% (84%)|  \n",
    "| ITERAB   | 6%  (2%)  | 60% (46%)|  \n",
    "| MINIMAX(750 ms)| 50% (54%)| --- |  \n",
    "| ITERAB(750 ms) | 8% (14%) | --- |  \n",
    "\n",
    "\n",
    "MCTS performs well versus the basic algos but is outperformed by both MINIMAX and ITERAB, the last by quite some distance. I was, honestly, a bit surprised that MCTS underperformed to this degree because its performance vs. the basics was similar. This simply indicates that performance vs. simple players doesn't give a very good measure of how well it will perform when facing very good agents. The performance of MCTS does improve a little if more processing time is allowed, but MINIMAX has a fixed depth of 3 for these tests so that may be an unfair comparison in that case.  \n",
    "\n",
    "My theory for the underperformance of MCTS is that the naive random rollout and the basic UCB for selection(i.e. no heuristics for guidance) requires too many select-expand-simulate-backprop steps to arrive at a good enough decision to beat ITERAB/MINIMAX.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c72e3-bf93-41d6-b64a-32f5e42def47",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
